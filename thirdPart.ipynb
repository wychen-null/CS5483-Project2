{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-22T03:24:05.703850Z",
     "start_time": "2024-04-22T03:24:05.002210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix as cm, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "random.seed(4487)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('data.pkl', 'rb') as file:\n",
    "    # 使用pickle的load方法从文件反序列化数据\n",
    "        data_loaded = pickle.load(file)\n",
    "# data_loaded现在是一个包含x，y，label的字典\n",
    "x = data_loaded['x']\n",
    "y = data_loaded['y']\n",
    "label = data_loaded['label']\n",
    "trainX, trainY, testX, testY = train_test_split(x, y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85b9bf67b14685de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ASVM 细节\n",
    "overview：我一共改了sigmoid和RBF的ASVM，我这里来说一下我的文档逻辑\n",
    "- 首先先介绍ASVM的理论框架\n",
    "- 改进思路\n",
    " 1. 并行计算：计算K-Fold的每一折使用joblib库进行并行，每一折都放在一个单独的CPU上运行，实测运行是从2500秒---->1700秒。\n",
    " 2. 动态调整步长\n",
    " 3. 自适应权重调整\n",
    " 4. 改进青蛙生成策略"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0399395ee08de4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  原始ASVM过程，你们可以参考 https://github.com/ElaineLIU-920/ASVM-for-Early-Cancer-Detection/blob/main/ASVM-Sigmoid.ipynb\n",
    "这里以Sigmoid核为例\n",
    "\n",
    "#  下面是sigmoid核的ASVM实现\n",
    "## 原始SVAM的重复性过程抽出定义为函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4898c7bb635fdcb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def SFLA_SVM(x_traincv, y_traincv,x_testcv, y_testcv,kernel,C,gamma=False,degree=False,coef0=False):\n",
    "    if C < 0:\n",
    "        C = random.randint(1,5) \n",
    "    clf = svm.SVC(C=C, kernel=kernel,gamma=gamma, coef0=coef0, probability=True,random_state=920).fit(x_traincv, y_traincv)\n",
    "    y_score = clf.predict_proba(x_testcv)[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_testcv, y_score, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr,tpr)\n",
    "    \n",
    "    return roc_auc"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce6a4fe7e47b2723"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def SFLA_SVM_CV(x_train, y_train,n,kernel,C,gamma=False,degree=False,coef0=False):\n",
    "    '''\n",
    "    n: number of splits for k-fold\n",
    "    \n",
    "    '''\n",
    "    KF = KFold(n_splits=n,shuffle=True, random_state=920)\n",
    "    f = []\n",
    "    for train_indexcv,test_indexcv in KF.split(x_train):\n",
    "        x_traincv, x_testcv = x_train.iloc[train_indexcv][:], x_train.iloc[test_indexcv][:]\n",
    "        y_traincv, y_testcv = y_train.iloc[train_indexcv][:], y_train.iloc[test_indexcv][:]\n",
    "        fq = SFLA_SVM(x_traincv, y_traincv,x_testcv, y_testcv,kernel,C,gamma=gamma,degree=degree,coef0=coef0) \n",
    "        f.append(fq) \n",
    "    f = mean(f)\n",
    "    return f"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b723d73f62ab947"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def check_within_bounds(Uq, rangeC, rangeGamma, rangeCoef0):\n",
    "    # 检查青蛙是否在可行的搜索空间内\n",
    "    return (rangeC[0] <= Uq[0] <= rangeC[1]) and \\\n",
    "           (rangeGamma[0] <= Uq[1] <= rangeGamma[1]) and \\\n",
    "           (rangeCoef0[0] <= Uq[2] <= rangeCoef0[1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e46917eb393b1f54"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_random_frog(rangeC, rangeGamma, rangeCoef0):\n",
    "    \"\"\"\n",
    "    Randomly generates a new frog within the feasible space.\n",
    "    \n",
    "    Parameters:\n",
    "    - rangeC: Tuple (min, max) for C parameter.\n",
    "    - rangeGamma: Tuple (min, max) for Gamma parameter.\n",
    "    - rangeCoef0: Tuple (min, max) for Coef0 parameter.\n",
    "    \n",
    "    Returns:\n",
    "    - Uq: A list containing the randomly generated C, Gamma, and Coef0.\n",
    "    \"\"\"\n",
    "    Uq = [\n",
    "        10**random.uniform(log10(rangeC[0]), log10(rangeC[1])),\n",
    "        10**random.uniform(log10(rangeGamma[0]), log10(rangeGamma[1])),\n",
    "        random.uniform(rangeCoef0[0], rangeCoef0[1])\n",
    "    ]\n",
    "    return Uq"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38c323b78676a2f0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def adaptative_gaussian_perturbation(Pw, frog_gb, iteration, max_iterations):\n",
    "    # 计算适应性因子，随着迭代进行，减小探索的随机性，alpha将更趋近于1，按本文的数据，最终alpha等于(1/e)\n",
    "    alpha = exp(-iteration / max_iterations)\n",
    "    # 用高斯分布代替均匀分布来生成随机数\n",
    "    gauss_factor = random.normal(1, 0.3)  # 均值1，标准差0.35\n",
    "    # 应用高斯扰动和适应性因子\n",
    "    Uq = Pw[1:] + alpha * gauss_factor * (frog_gb - Pw[1:])\n",
    "    return Uq"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4536d3360c8ae70b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模拟逆退火算法+适应性高斯插值改进最坏青蛙后以Sigmoid作为核的ASVM\n",
    "\n",
    "想理解代码，建议先把原来的思路看懂，然后再读我的代码，里面注释是相对比较详细的"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa2af05e9ced60ee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def SFLA_SIGMOID(num_parameter,num_global,num_local,m,n,q,n1,kernel,rangeC,rangeGamma,rangeCoef0,performance_history00,x_train,y_train):\n",
    "    '''\n",
    "    num_parameter: int, number of parameter to optimize\n",
    "    \n",
    "    num_global: int, the maximum number of global iterations\n",
    "    \n",
    "    num_local: int, the maximum number of local iterations\n",
    "    \n",
    "    m : int, the number of memeplexes\n",
    "    \n",
    "    n : int, the number of frogs in each memeplex\n",
    "    \n",
    "    q : int, the number of frogs in submemeplex\n",
    "    \n",
    "    n1:  number of splits for cross validation for inner loop\n",
    "    \n",
    "    rangeC: list, float, range of parameter C,eg.[10**-2, 10**2]\n",
    "    \n",
    "    rangeGamma: list, float, range of parameter Gamma,eg.[10**-6, 1]\n",
    "    \n",
    "    rangeCoef0: list, float, range of parameter Coef0,eg.[0, 1]\n",
    "\n",
    "    x_train: feature\n",
    "\n",
    "    y_train: lable\n",
    "\n",
    "    '''\n",
    "\n",
    "    #--- Step 0--Initialize parameters ---#\n",
    "    sizeC = 2\n",
    "    sizeGamma = 2\n",
    "    sizeCoef0 = 2\n",
    "    improvement_threshold = 10**-4 # 设置一个性能改进的阈值\n",
    "    performance_history = performance_history00 # 用于存储每只青蛙的历史性能和权重\n",
    "    global_flags = False\n",
    "    stop_main_loop = False\n",
    "    # max_step =  [(rangeC[1]-rangeC[0])/sizeC,(rangeGamma[1]-rangeGamma[0])/sizeGamma,(rangeCoef0[1]-rangeCoef0[0])/sizeCoef0]# maximum step size\n",
    "    initial_max_step =  [(rangeC[1]-rangeC[0])/sizeC,(rangeGamma[1]-rangeGamma[0])/sizeGamma,(rangeCoef0[1]-rangeCoef0[0])/sizeCoef0]# maximum step size\n",
    "    #--- Step 1--Generate initial population ---#\n",
    "    frogC = 10**random.uniform(log10(rangeC[0]),log10(rangeC[1]),m*n)\n",
    "    frogGamma = 10**random.uniform(log10(rangeGamma[0]),log10(rangeGamma[1]),m*n)\n",
    "    frogCoef0 = random.uniform(rangeCoef0[0],rangeCoef0[1],m*n)\n",
    "    frog = c_[frogC,frogGamma,frogCoef0]\n",
    "\n",
    "    # Compute the performance value for each frog on validation data #\n",
    "    KF = KFold(n_splits=n1,shuffle=True, random_state=920)\n",
    "    f = zeros((m*n,n1))\n",
    "    j = 0\n",
    "    for train_indexcv,test_indexcv in KF.split(x_train):\n",
    "        x_traincv, x_testcv = x_train.iloc[train_indexcv][:], x_train.iloc[test_indexcv][:]\n",
    "        y_traincv, y_testcv = y_train.iloc[train_indexcv][:], y_train.iloc[test_indexcv][:]\n",
    "        for i in range(m*n):\n",
    "            f[i,j] = SFLA_SVM(x_traincv, y_traincv,x_testcv, y_testcv,kernel,frog[i,0],frog[i,1],frog[i,2])\n",
    "        j+=1\n",
    "    f = f.mean(axis=1)\n",
    "    f_parameter = c_[f,frog]\n",
    "\n",
    "    #--- Step 2--Rank frogs ---#\n",
    "    f_parameter = f_parameter[argsort(f_parameter[:,0])[::-1]]\n",
    "\n",
    "    #######--- Global search start---######\n",
    "    i_global = 0\n",
    "    flag = 0\n",
    "    fBest_iteration = f_parameter[0,0]\n",
    "    weights = [2*(n+1-j)/(n*(n+1)) for j in range(1,n+1)] # weights of ranked frogs in each memeplex\n",
    "    while i_global < num_global:\n",
    "        # Dynamically adjust the maximum step size based on the iteration number\n",
    "        decay_factor = 0.89  # This factor determines how much the step size is reduced\n",
    "        max_step = [x * (decay_factor ** i_global) for x in initial_max_step]\n",
    "        frog_gb = f_parameter[0,0] # mark the global best frog      \n",
    "        #--- Step 3--Partition frogs into memeplexes ---#\n",
    "        memeplexes = zeros((m,n,num_parameter+1)) # [memeplexes, frog in memeplex,[f,C,Gamma,Coef0] ]\n",
    "        for i in range(m):\n",
    "            memeplexes[i] = f_parameter[linspace(i,m*n+i,num=n,endpoint=False,dtype=int)]\n",
    "\n",
    "       #######--- Global search start---######\n",
    "        i_global = 0\n",
    "        flag = 0\n",
    "        fBest_iteration = f_parameter[0,0]\n",
    "        iteration = 1              # 当前迭代次数高斯\n",
    "        max_iterations = 10        # 最大迭代次数，高斯和退火共享\n",
    "        iteration_sa = 1           # 当前迭代次数退火\n",
    "        # 初始化温度参数\n",
    "        initial_temp = 0.00004342944819032519\n",
    "        cooling_rate = 1.175  # 温度增长率（逆退火）\n",
    "        T = initial_temp  # 当前温度\n",
    "        global_flags = False\n",
    "        stop_main_loop = False\n",
    "        markf = 0\n",
    "        weights = [2*(n+1-j)/(n*(n+1)) for j in range(1,n+1)] # weights of ranked frogs in each memeplex\n",
    "        while i_global < num_global:\n",
    "            # Dynamically adjust the maximum step size based on the iteration number\n",
    "            decay_factor = 0.89  # This factor determines how much the step size is reduced\n",
    "            max_step = [x * (decay_factor ** i_global) for x in initial_max_step]\n",
    "            frog_gb = f_parameter[0,0] # mark the global best frog      \n",
    "            # Step 3: Partition frogs into memeplexes\n",
    "            memeplexes = zeros((m,n,num_parameter+1)) # [memeplexes, frog in memeplex,[f,C,Gamma,Coef0] ]\n",
    "            for i in range(m):\n",
    "                memeplexes[i] = f_parameter[linspace(i,m*n+i,num=n,endpoint=False,dtype=int)]\n",
    "            #######--- Local search start---######\n",
    "            # Step 4: Memetic evolution within each memeplex\n",
    "            im = 0 # the number of memeplexes that have been optimized\n",
    "            global_flags = False\n",
    "            stop_main_loop = False\n",
    "            markf = 0 # 这个就是标记是否在子复合体中找到了有效改进解，如果这个标记是1，说明没有找到\n",
    "            while im < m:\n",
    "                global_flags = False\n",
    "                stop_main_loop = False\n",
    "                i_local = 0 # counts the number of local evolutionary steps in each memeplex\n",
    "                while i_local < num_local:\n",
    "                    # Construct a submemeplex\n",
    "                    memeplex_indices = range(im * n, (im + 1) * n)\n",
    "                    if not all(performance_history[memeplex_indices, 1] == 0):\n",
    "                        rValue = random.random(q)\n",
    "                        memeplex_weights = performance_history[memeplex_indices, 1]\n",
    "                        random_indices = random.choice(memeplex_indices, size=q, p=memeplex_weights/sum(memeplex_weights), replace=False)\n",
    "                        selected_weights = performance_history[random_indices, 1]\n",
    "                        rValue *= selected_weights\n",
    "                        subindex = sort(argsort(rValue)[::-1][0:q])\n",
    "                    else:\n",
    "                        rValue = random.random(n)*weights \n",
    "                        subindex = sort(argsort(rValue)[::-1][0:q]) # index of selected frogs in memeplex \n",
    "                    submemeplex = memeplexes[im][subindex] # form submemeplex\n",
    "                    # Improve the worst frog's position using Simulated Annealing\n",
    "                    Pb = submemeplex[0]  # mark the best frog in submemeplex\n",
    "                    Pw = submemeplex[q-1]  # mark the worst frog in memeplex\n",
    "                    S = (Pb - Pw)[1:] * (Pb - Pw)[0]\n",
    "                    Uq = Pw[1:] + S\n",
    "                    # Check feasible space and the performance\n",
    "                    if check_within_bounds(Uq, rangeC, rangeGamma, rangeCoef0):\n",
    "                        fq = SFLA_SVM_CV(x_train, y_train, n1, kernel, Uq[0], Uq[1], Uq[2])\n",
    "                        if fq < Pw[0]:\n",
    "                            # If performance is not improved and we go the adaptative_gaussian_perturbation and annealing probability\n",
    "                            # 改进为使用结合模拟退火算法的高斯插值方法混合当前青蛙和全局最佳青蛙的参数，而不是完全随机生成\n",
    "                            # 注意这里退火算法我改了，评价指标改成一个单增的函数，就表示如果最坏青蛙向最好青蛙学习之后，如果性能下降了，但下降的越少的学习结果越容易被接受\n",
    "                            while iteration < max_iterations + 1 and not stop_main_loop:\n",
    "                                Uq = adaptative_gaussian_perturbation(Pw, frog_gb, iteration, max_iterations)\n",
    "                                if check_within_bounds(Uq, rangeC, rangeGamma, rangeCoef0):\n",
    "                                    fq = SFLA_SVM_CV(x_train, y_train, n1, kernel, Uq[0], Uq[1], Uq[2])\n",
    "                                    if fq < Pw[0]:#if performance is still not improved using SA algorithm 看看他们的差距是不是足够小\n",
    "                                        while iteration_sa < max_iterations + 1:\n",
    "                                            if random.rand() > exp(-(Pw[0] - fq) / T):#这个值是接受概率，如果和原始worst相差为0.002，那么逆退火算法的最大接受概率在0.4左右,\n",
    "                                                #如果相差在0.0015，那么最大接受概率约为0.5023096165445047，0.1~0.6318989908213715，这里就是为了带来随机性，增加鲁棒性\n",
    "                                                T *= cooling_rate #如果不被接受，那么降低温度\n",
    "                                                iteration_sa += 1\n",
    "                                            else:#一但有被接受的值，停止降温退出模拟退火算法\n",
    "                                                top_main_loop = True\n",
    "                                                global_flags = True\n",
    "                                                iteration_sa = 1\n",
    "                                                break\n",
    "                                        T = initial_temp #记得再次初始化T，不然T乘直接乘爆炸了\n",
    "                                        iteration_sa = 1\n",
    "                                iteration += 1\n",
    "                            if iteration == max_iterations + 1:\n",
    "                                markf = 1 #这就说明上面的没有找到局部改进解\n",
    "                            stop_main_loop = False    \n",
    "                            iteration = 1\n",
    "                        elif markf == 1: # if local parameter has not been improved then we go global for searching\n",
    "                            markf = 0\n",
    "                             # if searching has no result.get a new direction from the global best frog randomly\n",
    "                            S = random.random(num_parameter) * (frog_gb - Pw)[1:]\n",
    "                            for i in range(num_parameter):\n",
    "                                if S[i] > 0:\n",
    "                                    S[i] = min(S[i], max_step[i])\n",
    "                                else:\n",
    "                                    S[i] = min(S[i], -max_step[i])\n",
    "                            Uq = Pw[1:] + S\n",
    "                            if check_within_bounds(Uq, rangeC, rangeGamma, rangeCoef0):\n",
    "                                fq = SFLA_SVM_CV(x_train, y_train, n1, kernel, Uq[0], Uq[1], Uq[2])\n",
    "                                if fq < Pw[0]:\n",
    "                                # If performance is not improved and we go the adaptative_gaussian_perturbation and annealing probability\n",
    "                                    while iteration < max_iterations + 1 and not stop_main_loop:\n",
    "                                        Uq = adaptative_gaussian_perturbation(Pw, frog_gb, iteration, max_iterations)\n",
    "                                        if check_within_bounds(Uq, rangeC, rangeGamma, rangeCoef0):\n",
    "                                            fq = SFLA_SVM_CV(x_train, y_train, n1, kernel, Uq[0], Uq[1], Uq[2])\n",
    "                                            if fq < Pw[0]:#if performance is still not improved using SA algorithm 看看他们的差距是不是足够小\n",
    "                                                while iteration_sa < max_iterations + 1:\n",
    "                                                    if random.rand() > exp(-(Pw[0] - fq) / T) :#这个值是接受概率，如果和原始worst相差为0.002，那么逆退火算法的最大接受概率在0.4左右\n",
    "                                                        T *= cooling_rate #如果不被接受，那么降低温度\n",
    "                                                        iteration_sa += 1\n",
    "                                                    else:#一但有被接受的值，停止降温退出模拟退火算法\n",
    "                                                        top_main_loop = True\n",
    "                                                        global_flags = True\n",
    "                                                        iteration_sa = 1\n",
    "                                                        break\n",
    "                                                T = initial_temp #记得再次初始化T\n",
    "                                                iteration_sa = 1\n",
    "                                        iteration += 1\n",
    "                                    stop_main_loop = False\n",
    "                                    iteration = 1\n",
    "                            else:# 注意这里还是在前提，是在局部学习和全局学习都没有提升的前提下，这里还有一个前提是在局部学习是在可行界内的\n",
    "                                # If both local and global 's performances are not improved and new solution is not accepted based on annealing probability\n",
    "                                # Randomly generate a legal frog within the submemeplex\n",
    "                                Uq = generate_random_frog(rangeC, rangeGamma, rangeCoef0)\n",
    "                                fq = SFLA_SVM_CV(x_train, y_train, n1, kernel, Uq[0], Uq[1], Uq[2])\n",
    "                    else: # 如果局部学习直接不在可行解内了\n",
    "                        # If the worst frog 1 is beyond the search space, get a new direction from the global best frog for search\n",
    "                        S = random.random(num_parameter) * (frog_gb - Pw)[1:]\n",
    "                        for i in range(num_parameter):\n",
    "                            if S[i] > 0:\n",
    "                                S[i] = min(S[i], max_step[i])\n",
    "                            else:\n",
    "                                S[i] = min(S[i], -max_step[i])\n",
    "                        Uq = Pw[1:] + S\n",
    "                        if check_within_bounds(Uq, rangeC, rangeGamma, rangeCoef0):\n",
    "                            fq = SFLA_SVM_CV(x_train, y_train, n1, kernel, Uq[0], Uq[1], Uq[2])\n",
    "                            if fq < Pw[0]:\n",
    "                            # If performance is not improved and we go the adaptative_gaussian_perturbation and annealing probability\n",
    "                                while iteration < max_iterations + 1 and not stop_main_loop:\n",
    "                                    Uq = adaptative_gaussian_perturbation(Pw, frog_gb, iteration, max_iterations)\n",
    "                                    if check_within_bounds(Uq, rangeC, rangeGamma, rangeCoef0):\n",
    "                                        fq = SFLA_SVM_CV(x_train, y_train, n1, kernel, Uq[0], Uq[1], Uq[2])\n",
    "                                        if fq < Pw[0]:#if performance is still not improved using SA algorithm 看看他们的差距是不是足够小\n",
    "                                            while iteration_sa < max_iterations + 1:\n",
    "                                                if random.rand() > exp(-(Pw[0] - fq) / T) :#这个值是接受概率，如果和原始worst相差为0.002，那么逆退火算法的最大接受概率在0.4左右\n",
    "                                                    T *= cooling_rate #如果不被接受，那么降低温度\n",
    "                                                    iteration_sa += 1\n",
    "                                                else:#一但有被接受的值，停止降温退出模拟退火算法\n",
    "                                                    top_main_loop = True\n",
    "                                                    global_flags = True\n",
    "                                                    iteration_sa = 1\n",
    "                                                    break\n",
    "                                            T = initial_temp #记得再次初始化T\n",
    "                                            iteration_sa = 1\n",
    "                                    iteration += 1\n",
    "                                stop_main_loop = False\n",
    "                                iteration = 1\n",
    "                            else:\n",
    "                                # if global learning is not improved Randomly generate a legal frog within the submemeplex\n",
    "                                Uq = generate_random_frog(rangeC, rangeGamma, rangeCoef0)\n",
    "                                fq = SFLA_SVM_CV(x_train, y_train, n1, kernel, Uq[0], Uq[1], Uq[2])\n",
    "                        # if the space of the global learning is not feasible  Randomly generate a legal frog within the submemeplex\n",
    "                        else:\n",
    "                            Uq = generate_random_frog(rangeC, rangeGamma, rangeCoef0)\n",
    "                            fq = SFLA_SVM_CV(x_train, y_train, n1, kernel, Uq[0], Uq[1], Uq[2])\n",
    "#                     # Upgrade the memeplex\n",
    "#                     memeplexes[im][subindex[q-1]] = r_[fq, Uq]\n",
    "#                     memeplexes[im] = memeplexes[im][argsort(memeplexes[im][:,0])[::-1]]\n",
    "                    if fq > Pw[0] : # If there is performance improvement\n",
    "                        memeplexes[im][subindex[q-1]] = r_[fq,Uq]\n",
    "                        # Update the frog's historical performance and weight\n",
    "                        performance_history[subindex[q-1], 0] = fq\n",
    "                        performance_history[subindex[q-1], 1] += improvement_threshold # Increase weight\n",
    "                    elif global_flags == True : # If there is SA ACCECPT which means  the worst frog is closed to the best Within the tolerable performance degradation range\n",
    "                        # Update the frog's historical performance and weight\n",
    "                        performance_history[subindex[q-1], 0] = fq\n",
    "                        performance_history[subindex[q-1], 1] += improvement_threshold # Increase weight\n",
    "                        global_flags = False\n",
    "                    else:\n",
    "                        # If there is no performance improvement,and no SA ACCECPT, reduce the weight\n",
    "                        performance_history[subindex[q-1], 1] = max(performance_history[subindex[q-1], 1] - improvement_threshold, 0.1) # Ensure weight does not become zero\n",
    "                        #  By setting a minimum weight as 0.1, we ensure that every frog still has a chance to be selected, \n",
    "                        # but those with better performance have a higher probability.\n",
    "                    i_local += 1\n",
    "                im += 1\n",
    "            #######--- Local search end---######\n",
    "            # Step 5: Shuffle memeplexes\n",
    "            f_parameter = memeplexes.reshape(m*n,num_parameter+1)\n",
    "            f_parameter = f_parameter[argsort(f_parameter[:,0])[::-1]]\n",
    "            i_global += 1\n",
    "            # Step 6: Check convergence\n",
    "            if f_parameter[0,0] > 0.99:\n",
    "                print('The program was terminated because it reached the optimization goal with f = %.3f' %f_parameter[0,0])\n",
    "                break\n",
    "            if abs(frog_gb - f_parameter[0,0]) < 10**-4:\n",
    "                flag += 1\n",
    "            if flag > 5:\n",
    "                break\n",
    "            fBest_iteration = r_[fBest_iteration, f_parameter[0,0]]\n",
    "        #######--- Global search end---######\n",
    "        return (f_parameter[0], fBest_iteration)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90e3e2a5fd92549"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 调用搜索和迭代过程并行K-Fold运行"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T03:27:50.080643Z",
     "start_time": "2024-04-22T03:27:50.071428Z"
    }
   },
   "id": "cb1867c02b3414af",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(train_index, test_index, x, y, n1, kernel, rangeC, rangeGamma, rangeCoef0, num_parameter, num_global, num_local, m, n, q, performance_history):\n",
    "#     # 选取训练集和测试集\n",
    "#     x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "#     # 处理NaN值\n",
    "#     x_train.fillna(0, inplace=True)\n",
    "#     x_test.fillna(0, inplace=True)\n",
    "    #---  Seperate traing set and test set ---#\n",
    "    y_score = []\n",
    "    y_test = []\n",
    "    x_train, x_test = x.iloc[train_index][:], x.iloc[test_index][:]\n",
    "    y_train = y.iloc[train_index][:]\n",
    "        \n",
    "    #---  Fill NaN age ---#\n",
    "    x_train[isnan(x_train)] = 0\n",
    "    x_test[isnan(x_test)] = 0    \n",
    "        \n",
    "    ##---  optimize SVM with SFLA---##\n",
    "    x_train = pd.DataFrame(x_train) \n",
    "    y_train = pd.Series(y_train)\n",
    "    \n",
    "    if kernel == 'poly':\n",
    "        f_parameter,fBest_iteration = SFLA_POLY(num_parameter,num_global,num_local,m,n,q,n1,kernel,rangeC,rangeGamma,rangeDegree,rangeCoef0,x_train,y_train)\n",
    "        # f_parameter: list, [bestAUC,bestC,bestGamma,bestDegree,bestCoef0]   fBest_iteration: bestAUC in each iteration\n",
    "        ##---  creat and train the model ---##\n",
    "        clf = svm.SVC(kernel=kernel,C=f_parameter[1],gamma=f_parameter[2],degree=f_parameter[3],coef0=f_parameter[4],probability=True,random_state=920)\n",
    "            \n",
    "        \n",
    "    if kernel == 'rbf':\n",
    "        f_parameter,fBest_iteration = SFLA_RBF(num_parameter,num_global,num_local,m,n,q,n1,kernel,rangeC,rangeGamma,performance_history,x_train,y_train)\n",
    "        # f_parameter: list, [bestAUC,bestC,bestGamma,bestDegree,bestCoef0]   fBest_iteration: bestAUC in each iteration\n",
    "        \n",
    "        ##---  creat and train the model ---##\n",
    "        clf = svm.SVC(kernel=kernel,C=f_parameter[1],gamma=f_parameter[2],probability=True,random_state=920)\n",
    "        \n",
    "    if kernel == 'linear':\n",
    "        f_parameter,fBest_iteration = SFLA_LINEAR(num_parameter,num_global,num_local,m,n,q,n1,kernel,rangeC,x_train,y_train)\n",
    "         # f_parameter: list, [bestAUC,bestC,bestGamma,bestDegree,bestCoef0]   fBest_iteration: bestAUC in each iteration\n",
    "        \n",
    "        ##---  creat and train the model ---##\n",
    "        clf = svm.SVC(kernel=kernel,C=f_parameter[1],probability=True,random_state=920)\n",
    "        \n",
    "    if kernel == 'sigmoid':\n",
    "        f_parameter,fBest_iteration = SFLA_SIGMOID(num_parameter,num_global,num_local,m,n,q,n1,kernel,rangeC,rangeGamma,rangeCoef0,performance_history,x_train,y_train)\n",
    "        # f_parameter: list, [bestAUC,bestC,bestGamma,bestDegree,bestCoef0]   fBest_iteration: bestAUC in each iteration\n",
    "    \n",
    "        ##---  creat and train the model ---##\n",
    "        clf = svm.SVC(kernel=kernel,C=f_parameter[1],gamma=f_parameter[2],coef0=f_parameter[3],probability=True,random_state=920)\n",
    "\n",
    "    # 输出参数\n",
    "    print(f_parameter)\n",
    "    \n",
    "    # 创建并训练SVM模型\n",
    "    # clf = svm.SVC(kernel=kernel, C=f_parameter[1], gamma=f_parameter[2], coef0=f_parameter[3], probability=True, random_state=920)\n",
    "    clf.fit(x_train, y_train)\n",
    "    # Make predictions\n",
    "    y_score = clf.predict_proba(x_test)[:, 1]\n",
    "    y_test = y.iloc[test_index].tolist()\n",
    "    # Check if y_test and y_score have the same length\n",
    "    if len(y_test) != len(y_score):\n",
    "        raise ValueError(f\"Inconsistent number of samples: y_test has {len(y_test)} elements, y_score has {len(y_score)} elements\")\n",
    "\n",
    "    # Compute AUC\n",
    "    fpr, tpr, threshold = roc_curve(y_test, y_score, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print('AUC:', roc_auc)\n",
    "    \n",
    "    return y_score, y_test, roc_auc, clf\n",
    "    \n",
    "\n",
    "# 主函数\n",
    "def OptimizeSVM_SFLA_CV(x, y, n_splits, num_parameter, num_global, num_local, m, n, q, n1, kernel, rangeC, rangeGamma=False, rangeDegree=False, rangeCoef0=False):\n",
    "    performance_history = np.zeros((m*n, 2)) + 1.0\n",
    "    KF = KFold(n_splits=n_splits, shuffle=True, random_state=920)\n",
    "    y_score = []\n",
    "    y_test = []\n",
    "    # 使用joblib并行处理\n",
    "    results = Parallel(n_jobs=-1, verbose=10)(delayed(train_and_evaluate)(\n",
    "        train_index, test_index, x, y, n1, kernel, rangeC, rangeGamma, rangeCoef0, num_parameter, num_global, num_local, m, n, q, performance_history\n",
    "    ) for train_index, test_index in KF.split(x))\n",
    "\n",
    "    # 合并结果\n",
    "    y_scores = [score for result in results for score in result[0]]\n",
    "    y_tests = [y_test for result in results for y_test in result[1]]\n",
    "    roc_aucs = [result[2] for result in results]\n",
    "    clf = [result[3] for result in results]\n",
    "    \n",
    "\n",
    "    \n",
    "    # 输出整体AUC\n",
    "    fpr, tpr, threshold = roc_curve(y_tests, y_scores, pos_label=1)\n",
    "    plot_roc(y_tests, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # print('Overall AUC:', roc_auc)\n",
    "\n",
    "    # 计算其他性能指标\n",
    "    y_pred = [round(score) for score in y_scores]\n",
    "    print(cm(y_tests, y_pred))\n",
    "    a = accuracy_score(y_tests, y_pred)\n",
    "    p = precision_score(y_tests, y_pred)\n",
    "    r = recall_score(y_tests, y_pred)\n",
    "    f1score = f1_score(y_tests, y_pred)\n",
    "    print(f'Accuracy: {a:.2f}\\nPrecision: {p:.2f}\\nRecall: {r:.2f}\\nF1 Score: {f1score:.2f}\\nAUC: {roc_auc:.4f}\\n')\n",
    "\n",
    "    return clf, a, p, r, f1score, roc_auc, y_pred, y_scores, fpr, tpr"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b04041806152d68"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 如果希望测试代码迅速得到结果，请按我下面注释中的要求削减迭代次数规模"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b51821eb08ceb726"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "x_df = pd.DataFrame(x)\n",
    "y_s = pd.Series(y.ravel())  # 使用 ravel() 确保 y 是一维的\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "n_splits = 10 # 减少外循环的分割数\n",
    "num_parameter = 3\n",
    "num_global = 30 # 减少全局迭代的次数\n",
    "num_local = 20 # 减少局部迭代的次数\n",
    "m = 4 # 减少记忆复合体的数量\n",
    "n = 8 # 减少每个记忆复合体中的青蛙数量\n",
    "q = 5 # 减少子记忆复合体中的青蛙数量\n",
    "n1 = 10 # 减少内循环的分割数\n",
    "kernel = 'sigmoid'\n",
    "rangeC = [10**-12, 10**12] # 缩小参数C的范围\n",
    "rangeGamma = [10**-6, 1] # 缩小参数Gamma的范围\n",
    "rangeCoef0 = [0, 1] # 参数Coef0的范围保持不变\n",
    "clf, a, p, r, f1score,roc_auc, y_pred,y_score,fpr, tpr = OptimizeSVM_SFLA_CV(x_df,y_s,n_splits,num_parameter,num_global,num_local,m,n,q,n1,kernel,rangeC,rangeGamma,rangeCoef0=rangeCoef0)\n",
    "end = time.process_time()\n",
    "print('OptimizeSVM_SFLA_CV algorithm takes '+str(end - start)+'seconds.\\n') "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f862117b7fc34330"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
